{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ac7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "import fasttext\n",
    "import matplotlib.pyplot as plt\n",
    "from Clean_hindi_dataset import reader\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "print(\"Imported libraries...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921a7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------------------------\n",
    "### Constants\n",
    "###-----------------------------------\n",
    "\n",
    "EPOCHS = 10\n",
    "LR = 0.01\n",
    "DATASET_PATH = \"/home/dai001/Project/Dataset/clean-hindi-dataset\"\n",
    "SAVE_FOLDER = \"/home/dai001/Project/graphs/\"\n",
    "MODEL_PATH = \"/home/dai001/Project/model/transformer-model-best.pth\"\n",
    "MODEL_STATE_PATH = \"/home/dai001/Project/model/transformer-model-best-state.pth\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# parameters for Matplotlib\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 10),\n",
    "          'axes.labelsize': 'x-large',\n",
    "          'axes.titlesize':'x-large',\n",
    "          'xtick.labelsize':'x-large',\n",
    "          'ytick.labelsize':'x-large'\n",
    "         }\n",
    "\n",
    "CMAP = plt.cm.coolwarm\n",
    "\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Working on : \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b9de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------------------------\n",
    "### Positional Encoding class\n",
    "###-----------------------------------\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b15797",
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------------------------\n",
    "### Transformer Class\n",
    "###-----------------------------------\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, d_model, nhead, num_encoder_layers, num_decoder_layers,max_seq_len):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "        \n",
    "        self.dense = nn.Linear(d_model, embedding_matrix.size(0))\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        \n",
    "        src_embedding = self.embedding(src)\n",
    "        tgt_embedding = self.embedding(tgt)\n",
    "        #check if need for changing order of embedding shape\n",
    "        #src_embedding = src.permute(1, 0, 2)  # Permute to (seq_length, batch_size, embedding_dim)\n",
    "        #tgt_embedding = tgt.permute(1, 0, 2)\n",
    "        \n",
    "        src_embedding = self.positional_encoding(src)\n",
    "        tgt_embedding = self.positional_encoding(tgt)\n",
    "        \n",
    "        encoder_output = self.encoder(src_embedding)\n",
    "        decoder_output = self.decoder(tgt_embedding, encoder_output)\n",
    "        \n",
    "        #check for need if order is changed in previous step\n",
    "        #output = output.permute(1, 0, 2)  # Permute back to (batch_size, seq_length, embedding_dim)\n",
    "        output = self.dense(decoder_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326acb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained FastText Hindi embeddings\n",
    "embedding_loaded_model = fasttext.load_model(\"cc.hi.300.bin\")\n",
    "\n",
    "#Create the embedding matrix which contains all the embedding vectors for the entire vocabulary\n",
    "#Embedding matrix has each words in rows and its embeddings as columns(this model has 300 columns)\n",
    "embedding_matrix = torch.tensor(np.array([embedding_loaded_model.get_word_vector(word) for word in embedding_loaded_model.words]))\n",
    "\n",
    "#Create a lookup dictionary to convert words to indices\n",
    "word2idx = {word: idx for idx, word in enumerate(embedding_loaded_model.words)}\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "tokenizer = indic_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d60f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify all parameters which goes into the transformer model\n",
    "d_model = embedding_matrix.size(1)\n",
    "nhead = 10\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "max_seq_len = 100  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the model\n",
    "model = TransformerModel(d_model, nhead, num_encoder_layers, num_decoder_layers,max_seq_len).to(device)\n",
    "print(\"MODEL:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f1a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------------------------\n",
    "### Custom Dataset Class\n",
    "###-----------------------------------\n",
    "\n",
    "class DS(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_seq_length):\n",
    "        self.file_path = file_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_length\n",
    "        self.data = []\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as fp:\n",
    "            for s in reader(fp):\n",
    "                if s:\n",
    "                    self.data.append(s)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        text = self.data[idx]\n",
    "        tokens = self.tokenizer.trivial_tokenize(text)[:self.max_seq_len]\n",
    "        \n",
    "        # Taking random size of words from the text to train instead of the entire sentence\n",
    "        if len(tokens)>3:\n",
    "            random_size = random.randint(3,len(tokens))\n",
    "            random_start = random.randint(0,len(tokens)-1)\n",
    "            selected = np.array(tokens[random_start:random_start+random_size])\n",
    "            target = selected[-1]\n",
    "            features = selected[:-1]\n",
    "        else:\n",
    "            features = np.array(tokens[:-1])\n",
    "            target = tokens[-1]\n",
    "        # Converting into tensors    \n",
    "        features = torch.tensor(features)\n",
    "        target = torch.tensor(target)\n",
    "\n",
    "        return features,target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c13e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "train_ds = DS(DATASET_PATH+\"-train\", tokenizer, max_seq_len)\n",
    "test_ds = DS(DATASET_PATH+\"-test\", tokenizer, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d36c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloader\n",
    "train_dl = DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_dl = DataLoader(test_ds,batch_size=BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1055817",
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------------------------\n",
    "### Training Model\n",
    "###-----------------------------------\n",
    "\n",
    "print(\"Beginning Training...\")\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "best_acc = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, target in train_dl:\n",
    "        #Embedding features and target\n",
    "        features, target = embed(features, target)\n",
    "        \n",
    "        #Sending features and target to gpu\n",
    "        features, target = features.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass through model\n",
    "        output = model(features, target)  \n",
    "\n",
    "        # Reshape the output and target tensors to calculate loss\n",
    "        output = output.view(-1, embedding_matrix.size(0))\n",
    "        target = target.view(-1)\n",
    "\n",
    "        loss = loss_fn(output, target)\n",
    "        acc = accuracy_score(target.cpu().numpy(),output.cpu().numpy())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dl)\n",
    "    avg_train_acc = total_acc / len(train_dl)\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accs.append(avg_train_acc)\n",
    "    \n",
    "\n",
    "    # Testing loop\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for features, target in test_dl:\n",
    "            features, target = features.to(device), target.to(device)\n",
    "\n",
    "            output = model(features, features)  # Auto-regressive prediction\n",
    "\n",
    "            output = output.view(-1, embedding_matrix.size(0))\n",
    "            target = target.view(-1)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            acc = accuracy_score(target.cpu().numpy(),output.cpu().numpy())\n",
    "            \n",
    "            total_test_loss += loss.item()\n",
    "            total_test_acc += acc()\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_dl)\n",
    "    avg_test_acc = total_test_acc / len(test_dl)\n",
    "    \n",
    "    if avg_test_acc > best_acc:\n",
    "        torch.save(model,MODEL_PATH)\n",
    "        torch.save(model.state_dict(),MODEL_STATE_PATH)\n",
    "    \n",
    "    test_losses.append(avg_test_loss)\n",
    "    test_accs.append(avg_test_acc)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {avg_train_loss:.4f} - Test Loss: {avg_test_loss:.4f} - Train Acc: {avg_train_acc:.4f} - Test Acc: {avg_test_acc:.4f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f874f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------------------------\n",
    "### Function to plot Loss Curve\n",
    "###-----------------------------------\n",
    "\n",
    "\n",
    "def plot_hist(hist_df, save_folder):\n",
    "    # instantiate figure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # properties  matplotlib.patch.Patch\n",
    "    props = dict(boxstyle='round', facecolor='cyan', alpha=0.5)\n",
    "\n",
    "    # Where was min loss\n",
    "    best = hist_df[hist_df['test_loss'] == hist_df['test_loss'].min()]\n",
    "\n",
    "    # pick first axis\n",
    "    ax = axes[0]\n",
    "\n",
    "    # Plot all losses\n",
    "    hist_df.plot(x='epoch', y=['loss', 'test_loss'], ax=ax)\n",
    "\n",
    "    # little beautification\n",
    "    txtFmt = \"Loss: \\n  train: {:6.4f}\\n   test: {:6.4f}\"\n",
    "    txtstr = txtFmt.format(hist_df.iloc[-1]['loss'],\n",
    "                           hist_df.iloc[-1]['test_loss'])  # text to plot\n",
    "\n",
    "    # place a text box in upper middle in axes coords\n",
    "    ax.text(0.3, 0.95, txtstr, transform=ax.transAxes, fontsize=14,\n",
    "            verticalalignment='top', bbox=props)\n",
    "\n",
    "    # Mark arrow at lowest\n",
    "    ax.annotate(f'Min: {best[\"test_loss\"].to_numpy()[0]:6.4f}',  # text to print\n",
    "                xy=(best['epoch'].to_numpy(), best[\"test_loss\"].to_numpy()[0]),  # Arrow start\n",
    "                xytext=(best['epoch'].to_numpy() + 0.01, best[\"test_loss\"].to_numpy()[0] + 0.01),  # location of text\n",
    "                fontsize=14, va='bottom', ha='right', bbox=props,  # beautification of text\n",
    "                arrowprops=dict(facecolor='cyan', shrink=0.05))  # arrow\n",
    "\n",
    "    # Draw vertical line at best value\n",
    "    ax.axvline(x=best['epoch'].to_numpy(), color='green', linestyle='-.', lw=3)\n",
    "\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title('Errors')\n",
    "    ax.grid()\n",
    "    ax.legend(loc='upper left')  # model legend to upper left\n",
    "\n",
    "    # pick second axis\n",
    "    ax = axes[1]\n",
    "\n",
    "    # Plot accuracies\n",
    "    hist_df.plot(x='epoch', y=['acc', 'test_acc'], ax=ax)\n",
    "\n",
    "    # little beautification\n",
    "    txtFmt = \"Accuracy: \\n  train: {:6.4f}\\n  test:  {:6.4f}\"\n",
    "    txtstr = txtFmt.format(hist_df.iloc[-1]['acc'],\n",
    "                           hist_df.iloc[-1]['test_acc'])  # text to plot\n",
    "\n",
    "    # place a text box in lower middle in axes coords\n",
    "    ax.text(0.3, 0.2, txtstr, transform=ax.transAxes, fontsize=12,\n",
    "            verticalalignment='top', bbox=props)\n",
    "\n",
    "    # Mark arrow at lowest\n",
    "    ax.annotate(f'Best: {best[\"test_acc\"].to_numpy()[0]:6.4f}',  # text to print\n",
    "                xy=(best['epoch'].to_numpy(), best[\"test_acc\"].to_numpy()[0]),  # Arrow start\n",
    "                xytext=(best['epoch'].to_numpy() - 2, best[\"test_acc\"].to_numpy()[0]),  # location of text\n",
    "                fontsize=14, va='bottom', ha='right', bbox=props,  # beautification of text\n",
    "                arrowprops=dict(facecolor='cyan', shrink=0.05))  # arrow\n",
    "\n",
    "    # Draw a vertical line at best value\n",
    "    ax.axvline(x=best['epoch'].to_numpy(),\n",
    "               color='green',\n",
    "               linestyle='-.', lw=3)\n",
    "\n",
    "    # Labels\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title('Accuracies')\n",
    "    ax.grid()\n",
    "    ax.legend(loc='lower left')\n",
    "\n",
    "    # Save the figure instead of displaying\n",
    "    save_path = os.path.join(SAVE_FOLDER, \"loss_and_accuracy_plot.png\")\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    # Close the figure to free up resources\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b242ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the loss and accuracy\n",
    "print(\"Plotting the loss and accuracy curves...\")\n",
    "loss_df = pd.DataFrame({'epoch' : n_epoch, 'loss' : loss, 'test_loss': tloss, 'acc' : acc, 'test_acc': tacc})\n",
    "plot_hist(loss_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
